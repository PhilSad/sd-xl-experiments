{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5576fc50-6464-4ea1-9ba7-34773b144500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'sd-scripts'...\n",
      "remote: Enumerating objects: 3719, done.\u001b[K\n",
      "remote: Counting objects: 100% (2258/2258), done.\u001b[K\n",
      "remote: Compressing objects: 100% (371/371), done.\u001b[K\n",
      "remote: Total 3719 (delta 1987), reused 2072 (delta 1887), pack-reused 1461\u001b[K\n",
      "Receiving objects: 100% (3719/3719), 3.63 MiB | 18.59 MiB/s, done.\n",
      "Resolving deltas: 100% (2609/2609), done.\n",
      "Branch 'sdxl' set up to track remote branch 'sdxl' from 'origin'.\n",
      "Switched to a new branch 'sdxl'\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/kohya-ss/sd-scripts/\n",
    "!cd sd-scripts && git checkout sdxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b689dda-992c-41ac-8325-90258539ec45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xformers in /usr/local/lib/python3.10/dist-packages (0.0.20)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.15.7-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.24.1)\n",
      "Requirement already satisfied: pyre-extensions==0.0.29 in /usr/local/lib/python3.10/dist-packages (from xformers) (0.0.29)\n",
      "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.0.1+cu118)\n",
      "Requirement already satisfied: typing-inspect in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers) (4.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (15.0.7)\n",
      "Collecting Click!=8.0.0,>=7.1 (from wandb)\n",
      "  Downloading click-8.1.6-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.28.1-py2.py3-none-any.whl (214 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.7/214.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
      "Collecting pathtools (from wandb)\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.0.0)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.19.6)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->xformers) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->xformers) (1.2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect->pyre-extensions==0.0.29->xformers) (1.0.0)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=f95b118e5d72229511b780ac697618455b6cd9c1b6e293c1b9b3b04a5fe24751\n",
      "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
      "Successfully built pathtools\n",
      "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, Click, gitdb, GitPython, wandb\n",
      "Successfully installed Click-8.1.6 GitPython-3.1.32 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.28.1 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xformers wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7b1840-c58e-451f-b2cd-d4105de25da5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///workspace/sd-xl-experiments/lora/sd-scripts (from -r requirements.txt (line 29))\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting accelerate==0.19.0 (from -r requirements.txt (line 1))\n",
      "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.30.2 (from -r requirements.txt (line 2))\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting diffusers[torch]==0.18.2 (from -r requirements.txt (line 3))\n",
      "  Downloading diffusers-0.18.2-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting ftfy==6.1.1 (from -r requirements.txt (line 4))\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting albumentations==1.3.0 (from -r requirements.txt (line 5))\n",
      "  Downloading albumentations-1.3.0-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opencv-python==4.7.0.68 (from -r requirements.txt (line 6))\n",
      "  Downloading opencv_python-4.7.0.68-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting einops==0.6.0 (from -r requirements.txt (line 7))\n",
      "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytorch-lightning==1.9.0 (from -r requirements.txt (line 8))\n",
      "  Downloading pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes==0.35.0 (from -r requirements.txt (line 9))\n",
      "  Downloading bitsandbytes-0.35.0-py3-none-any.whl (62.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard==2.10.1 (from -r requirements.txt (line 10))\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors==0.3.1 (from -r requirements.txt (line 11))\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting altair==4.2.2 (from -r requirements.txt (line 13))\n",
      "  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.6/813.6 kB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting easygui==0.98.3 (from -r requirements.txt (line 14))\n",
      "  Downloading easygui-0.98.3-py2.py3-none-any.whl (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting toml==0.10.2 (from -r requirements.txt (line 15))\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting voluptuous==0.13.1 (from -r requirements.txt (line 16))\n",
      "  Downloading voluptuous-0.13.1-py3-none-any.whl (29 kB)\n",
      "Collecting huggingface-hub==0.15.1 (from -r requirements.txt (line 17))\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting invisible-watermark==0.2.0 (from -r requirements.txt (line 19))\n",
      "  Downloading invisible_watermark-0.2.0-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting open-clip-torch==2.20.0 (from -r requirements.txt (line 27))\n",
      "  Downloading open_clip_torch-2.20.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.19.0->-r requirements.txt (line 1)) (2.0.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (3.9.0)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.30.2->-r requirements.txt (line 2))\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.28.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2->-r requirements.txt (line 2))\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers[torch]==0.18.2->-r requirements.txt (line 3)) (9.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers[torch]==0.18.2->-r requirements.txt (line 3)) (4.6.4)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy==6.1.1->-r requirements.txt (line 4)) (0.2.6)\n",
      "Collecting scipy (from albumentations==1.3.0->-r requirements.txt (line 5))\n",
      "  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-image>=0.16.1 (from albumentations==1.3.0->-r requirements.txt (line 5))\n",
      "  Downloading scikit_image-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting qudida>=0.0.4 (from albumentations==1.3.0->-r requirements.txt (line 5))\n",
      "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
      "Collecting opencv-python-headless>=4.1.1 (from albumentations==1.3.0->-r requirements.txt (line 5))\n",
      "  Downloading opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>2021.06.0 (from pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchmetrics>=0.7.0 (from pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Downloading torchmetrics-1.0.1-py3-none-any.whl (729 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m729.2/729.2 kB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.9.0->-r requirements.txt (line 8)) (4.4.0)\n",
      "Collecting lightning-utilities>=0.4.2 (from pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard==2.10.1->-r requirements.txt (line 10))\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio>=1.24.3 (from tensorboard==2.10.1->-r requirements.txt (line 10))\n",
      "  Downloading grpcio-1.56.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3 (from tensorboard==2.10.1->-r requirements.txt (line 10))\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard==2.10.1->-r requirements.txt (line 10))\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard==2.10.1->-r requirements.txt (line 10))\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<3.20,>=3.9.2 (from tensorboard==2.10.1->-r requirements.txt (line 10))\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.10.1->-r requirements.txt (line 10)) (68.0.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard==2.10.1->-r requirements.txt (line 10))\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard==2.10.1->-r requirements.txt (line 10))\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1 (from tensorboard==2.10.1->-r requirements.txt (line 10))\n",
      "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.10.1->-r requirements.txt (line 10)) (0.40.0)\n",
      "Collecting entrypoints (from altair==4.2.2->-r requirements.txt (line 13))\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair==4.2.2->-r requirements.txt (line 13)) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair==4.2.2->-r requirements.txt (line 13)) (4.18.0)\n",
      "Collecting pandas>=0.18 (from altair==4.2.2->-r requirements.txt (line 13))\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting toolz (from altair==4.2.2->-r requirements.txt (line 13))\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting PyWavelets>=1.1.1 (from invisible-watermark==0.2.0->-r requirements.txt (line 19))\n",
      "  Downloading PyWavelets-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open-clip-torch==2.20.0->-r requirements.txt (line 27)) (0.15.2+cu118)\n",
      "Collecting sentencepiece (from open-clip-torch==2.20.0->-r requirements.txt (line 27))\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting timm (from open-clip-torch==2.20.0->-r requirements.txt (line 27))\n",
      "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>2021.06.0->pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r requirements.txt (line 10))\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r requirements.txt (line 10))\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r requirements.txt (line 10))\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r requirements.txt (line 10)) (1.26.13)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.10.1->-r requirements.txt (line 10))\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair==4.2.2->-r requirements.txt (line 13)) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair==4.2.2->-r requirements.txt (line 13)) (2023.6.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair==4.2.2->-r requirements.txt (line 13)) (0.29.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair==4.2.2->-r requirements.txt (line 13)) (0.8.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->altair==4.2.2->-r requirements.txt (line 13)) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas>=0.18->altair==4.2.2->-r requirements.txt (line 13))\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1 (from pandas>=0.18->altair==4.2.2->-r requirements.txt (line 13))\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn>=0.19.1 (from qudida>=0.0.4->albumentations==1.3.0->-r requirements.txt (line 5))\n",
      "  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2022.12.7)\n",
      "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0->-r requirements.txt (line 5)) (3.0)\n",
      "Collecting imageio>=2.27 (from scikit-image>=0.16.1->albumentations==1.3.0->-r requirements.txt (line 5))\n",
      "  Downloading imageio-2.31.1-py3-none-any.whl (313 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tifffile>=2022.8.12 (from scikit-image>=0.16.1->albumentations==1.3.0->-r requirements.txt (line 5))\n",
      "  Downloading tifffile-2023.7.18-py3-none-any.whl (221 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lazy_loader>=0.2 (from scikit-image>=0.16.1->albumentations==1.3.0->-r requirements.txt (line 5))\n",
      "  Downloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.19.0->-r requirements.txt (line 1)) (1.11.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.19.0->-r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate==0.19.0->-r requirements.txt (line 1)) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate==0.19.0->-r requirements.txt (line 1)) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard==2.10.1->-r requirements.txt (line 10)) (2.1.2)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0->-r requirements.txt (line 8))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.10.1->-r requirements.txt (line 10))\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.10.1->-r requirements.txt (line 10)) (3.2.0)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0->-r requirements.txt (line 5))\n",
      "  Downloading joblib-1.3.1-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0->-r requirements.txt (line 5))\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate==0.19.0->-r requirements.txt (line 1)) (1.2.1)\n",
      "Installing collected packages: voluptuous, tokenizers, tensorboard-plugin-wit, sentencepiece, safetensors, pytz, library, easygui, bitsandbytes, werkzeug, tzdata, toolz, toml, tifffile, threadpoolctl, tensorboard-data-server, scipy, regex, PyWavelets, pyasn1, protobuf, opencv-python-headless, opencv-python, multidict, markdown, lightning-utilities, lazy_loader, joblib, imageio, grpcio, ftfy, fsspec, frozenlist, entrypoints, einops, cachetools, async-timeout, absl-py, yarl, scikit-learn, scikit-image, rsa, requests-oauthlib, pyasn1-modules, pandas, huggingface-hub, aiosignal, transformers, qudida, google-auth, diffusers, aiohttp, google-auth-oauthlib, altair, albumentations, tensorboard, torchmetrics, timm, accelerate, pytorch-lightning, open-clip-torch, invisible-watermark\n",
      "  Running setup.py develop for library\n",
      "Successfully installed PyWavelets-1.4.1 absl-py-1.4.0 accelerate-0.19.0 aiohttp-3.8.5 aiosignal-1.3.1 albumentations-1.3.0 altair-4.2.2 async-timeout-4.0.2 bitsandbytes-0.35.0 cachetools-5.3.1 diffusers-0.18.2 easygui-0.98.3 einops-0.6.0 entrypoints-0.4 frozenlist-1.4.0 fsspec-2023.6.0 ftfy-6.1.1 google-auth-2.22.0 google-auth-oauthlib-0.4.6 grpcio-1.56.2 huggingface-hub-0.15.1 imageio-2.31.1 invisible-watermark-0.2.0 joblib-1.3.1 lazy_loader-0.3 library-0.0.0 lightning-utilities-0.9.0 markdown-3.4.4 multidict-6.0.4 open-clip-torch-2.20.0 opencv-python-4.7.0.68 opencv-python-headless-4.8.0.74 pandas-2.0.3 protobuf-3.19.6 pyasn1-0.5.0 pyasn1-modules-0.3.0 pytorch-lightning-1.9.0 pytz-2023.3 qudida-0.0.4 regex-2023.6.3 requests-oauthlib-1.3.1 rsa-4.9 safetensors-0.3.1 scikit-image-0.21.0 scikit-learn-1.3.0 scipy-1.11.1 sentencepiece-0.1.99 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 threadpoolctl-3.2.0 tifffile-2023.7.18 timm-0.9.2 tokenizers-0.13.3 toml-0.10.2 toolz-0.12.0 torchmetrics-1.0.1 transformers-4.30.2 tzdata-2023.3 voluptuous-0.13.1 werkzeug-2.3.6 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd sd-scripts/ && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c91ce1d5-cbaf-4115-bc16-eccddd75ffc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sdxl_train_network.py [-h] [--v2] [--v_parameterization]\n",
      "                             [--pretrained_model_name_or_path PRETRAINED_MODEL_NAME_OR_PATH]\n",
      "                             [--tokenizer_cache_dir TOKENIZER_CACHE_DIR]\n",
      "                             [--train_data_dir TRAIN_DATA_DIR]\n",
      "                             [--shuffle_caption]\n",
      "                             [--caption_extension CAPTION_EXTENSION]\n",
      "                             [--caption_extention CAPTION_EXTENTION]\n",
      "                             [--keep_tokens KEEP_TOKENS] [--color_aug]\n",
      "                             [--flip_aug]\n",
      "                             [--face_crop_aug_range FACE_CROP_AUG_RANGE]\n",
      "                             [--random_crop] [--debug_dataset]\n",
      "                             [--resolution RESOLUTION] [--cache_latents]\n",
      "                             [--vae_batch_size VAE_BATCH_SIZE]\n",
      "                             [--cache_latents_to_disk] [--enable_bucket]\n",
      "                             [--min_bucket_reso MIN_BUCKET_RESO]\n",
      "                             [--max_bucket_reso MAX_BUCKET_RESO]\n",
      "                             [--bucket_reso_steps BUCKET_RESO_STEPS]\n",
      "                             [--bucket_no_upscale]\n",
      "                             [--token_warmup_min TOKEN_WARMUP_MIN]\n",
      "                             [--token_warmup_step TOKEN_WARMUP_STEP]\n",
      "                             [--dataset_class DATASET_CLASS]\n",
      "                             [--caption_dropout_rate CAPTION_DROPOUT_RATE]\n",
      "                             [--caption_dropout_every_n_epochs CAPTION_DROPOUT_EVERY_N_EPOCHS]\n",
      "                             [--caption_tag_dropout_rate CAPTION_TAG_DROPOUT_RATE]\n",
      "                             [--reg_data_dir REG_DATA_DIR] [--in_json IN_JSON]\n",
      "                             [--dataset_repeats DATASET_REPEATS]\n",
      "                             [--output_dir OUTPUT_DIR]\n",
      "                             [--output_name OUTPUT_NAME]\n",
      "                             [--huggingface_repo_id HUGGINGFACE_REPO_ID]\n",
      "                             [--huggingface_repo_type HUGGINGFACE_REPO_TYPE]\n",
      "                             [--huggingface_path_in_repo HUGGINGFACE_PATH_IN_REPO]\n",
      "                             [--huggingface_token HUGGINGFACE_TOKEN]\n",
      "                             [--huggingface_repo_visibility HUGGINGFACE_REPO_VISIBILITY]\n",
      "                             [--save_state_to_huggingface]\n",
      "                             [--resume_from_huggingface] [--async_upload]\n",
      "                             [--save_precision {None,float,fp16,bf16}]\n",
      "                             [--save_every_n_epochs SAVE_EVERY_N_EPOCHS]\n",
      "                             [--save_every_n_steps SAVE_EVERY_N_STEPS]\n",
      "                             [--save_n_epoch_ratio SAVE_N_EPOCH_RATIO]\n",
      "                             [--save_last_n_epochs SAVE_LAST_N_EPOCHS]\n",
      "                             [--save_last_n_epochs_state SAVE_LAST_N_EPOCHS_STATE]\n",
      "                             [--save_last_n_steps SAVE_LAST_N_STEPS]\n",
      "                             [--save_last_n_steps_state SAVE_LAST_N_STEPS_STATE]\n",
      "                             [--save_state] [--resume RESUME]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--max_token_length {None,150,225}]\n",
      "                             [--mem_eff_attn] [--xformers] [--sdpa]\n",
      "                             [--vae VAE] [--max_train_steps MAX_TRAIN_STEPS]\n",
      "                             [--max_train_epochs MAX_TRAIN_EPOCHS]\n",
      "                             [--max_data_loader_n_workers MAX_DATA_LOADER_N_WORKERS]\n",
      "                             [--persistent_data_loader_workers] [--seed SEED]\n",
      "                             [--gradient_checkpointing]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--mixed_precision {no,fp16,bf16}] [--full_fp16]\n",
      "                             [--full_bf16] [--clip_skip CLIP_SKIP]\n",
      "                             [--logging_dir LOGGING_DIR]\n",
      "                             [--log_with {tensorboard,wandb,all}]\n",
      "                             [--log_prefix LOG_PREFIX]\n",
      "                             [--log_tracker_name LOG_TRACKER_NAME]\n",
      "                             [--log_tracker_config LOG_TRACKER_CONFIG]\n",
      "                             [--wandb_api_key WANDB_API_KEY]\n",
      "                             [--noise_offset NOISE_OFFSET]\n",
      "                             [--multires_noise_iterations MULTIRES_NOISE_ITERATIONS]\n",
      "                             [--multires_noise_discount MULTIRES_NOISE_DISCOUNT]\n",
      "                             [--adaptive_noise_scale ADAPTIVE_NOISE_SCALE]\n",
      "                             [--zero_terminal_snr]\n",
      "                             [--min_timestep MIN_TIMESTEP]\n",
      "                             [--max_timestep MAX_TIMESTEP] [--lowram]\n",
      "                             [--sample_every_n_steps SAMPLE_EVERY_N_STEPS]\n",
      "                             [--sample_every_n_epochs SAMPLE_EVERY_N_EPOCHS]\n",
      "                             [--sample_prompts SAMPLE_PROMPTS]\n",
      "                             [--sample_sampler {ddim,pndm,lms,euler,euler_a,heun,dpm_2,dpm_2_a,dpmsolver,dpmsolver++,dpmsingle,k_lms,k_euler,k_euler_a,k_dpm_2,k_dpm_2_a}]\n",
      "                             [--config_file CONFIG_FILE] [--output_config]\n",
      "                             [--prior_loss_weight PRIOR_LOSS_WEIGHT]\n",
      "                             [--optimizer_type OPTIMIZER_TYPE]\n",
      "                             [--use_8bit_adam] [--use_lion_optimizer]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--optimizer_args [OPTIMIZER_ARGS ...]]\n",
      "                             [--lr_scheduler_type LR_SCHEDULER_TYPE]\n",
      "                             [--lr_scheduler_args [LR_SCHEDULER_ARGS ...]]\n",
      "                             [--lr_scheduler LR_SCHEDULER]\n",
      "                             [--lr_warmup_steps LR_WARMUP_STEPS]\n",
      "                             [--lr_scheduler_num_cycles LR_SCHEDULER_NUM_CYCLES]\n",
      "                             [--lr_scheduler_power LR_SCHEDULER_POWER]\n",
      "                             [--dataset_config DATASET_CONFIG]\n",
      "                             [--min_snr_gamma MIN_SNR_GAMMA]\n",
      "                             [--scale_v_pred_loss_like_noise_pred]\n",
      "                             [--weighted_captions] [--no_metadata]\n",
      "                             [--save_model_as {None,ckpt,pt,safetensors}]\n",
      "                             [--unet_lr UNET_LR]\n",
      "                             [--text_encoder_lr TEXT_ENCODER_LR]\n",
      "                             [--network_weights NETWORK_WEIGHTS]\n",
      "                             [--network_module NETWORK_MODULE]\n",
      "                             [--network_dim NETWORK_DIM]\n",
      "                             [--network_alpha NETWORK_ALPHA]\n",
      "                             [--network_dropout NETWORK_DROPOUT]\n",
      "                             [--network_args [NETWORK_ARGS ...]]\n",
      "                             [--network_train_unet_only]\n",
      "                             [--network_train_text_encoder_only]\n",
      "                             [--training_comment TRAINING_COMMENT]\n",
      "                             [--dim_from_weights]\n",
      "                             [--scale_weight_norms SCALE_WEIGHT_NORMS]\n",
      "                             [--base_weights [BASE_WEIGHTS ...]]\n",
      "                             [--base_weights_multiplier [BASE_WEIGHTS_MULTIPLIER ...]]\n",
      "                             [--no_half_vae] [--cache_text_encoder_outputs]\n",
      "                             [--cache_text_encoder_outputs_to_disk]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --v2                  load Stable Diffusion v2.0 model / Stable Diffusion\n",
      "                        2.0のモデルを読み込む\n",
      "  --v_parameterization  enable v-parameterization training /\n",
      "                        v-parameterization学習を有効にする\n",
      "  --pretrained_model_name_or_path PRETRAINED_MODEL_NAME_OR_PATH\n",
      "                        pretrained model to train, directory to Diffusers\n",
      "                        model or StableDiffusion checkpoint / 学習元モデル、Diffusers\n",
      "                        形式モデルのディレクトリまたはStableDiffusionのckptファイル\n",
      "  --tokenizer_cache_dir TOKENIZER_CACHE_DIR\n",
      "                        directory for caching Tokenizer (for offline training)\n",
      "                        / Tokenizerをキャッシュするディレクトリ（ネット接続なしでの学習のため）\n",
      "  --train_data_dir TRAIN_DATA_DIR\n",
      "                        directory for train images / 学習画像データのディレクトリ\n",
      "  --shuffle_caption     shuffle comma-separated caption /\n",
      "                        コンマで区切られたcaptionの各要素をshuffleする\n",
      "  --caption_extension CAPTION_EXTENSION\n",
      "                        extension of caption files / 読み込むcaptionファイルの拡張子\n",
      "  --caption_extention CAPTION_EXTENTION\n",
      "                        extension of caption files (backward compatibility) /\n",
      "                        読み込むcaptionファイルの拡張子（スペルミスを残してあります）\n",
      "  --keep_tokens KEEP_TOKENS\n",
      "                        keep heading N tokens when shuffling caption tokens\n",
      "                        (token means comma separated strings) / captionのシャッフル時\n",
      "                        に、先頭からこの個数のトークンをシャッフルしないで残す（トークンはカンマ区切りの各部分を意味する）\n",
      "  --color_aug           enable weak color augmentation /\n",
      "                        学習時に色合いのaugmentationを有効にする\n",
      "  --flip_aug            enable horizontal flip augmentation /\n",
      "                        学習時に左右反転のaugmentationを有効にする\n",
      "  --face_crop_aug_range FACE_CROP_AUG_RANGE\n",
      "                        enable face-centered crop augmentation and its range\n",
      "                        (e.g. 2.0,4.0) /\n",
      "                        学習時に顔を中心とした切り出しaugmentationを有効にするときは倍率を指定する（例：2.0,4.0）\n",
      "  --random_crop         enable random crop (for style training in face-\n",
      "                        centered crop augmentation) /\n",
      "                        ランダムな切り出しを有効にする（顔を中心としたaugmentationを行うときに画風の学習用に指定する）\n",
      "  --debug_dataset       show images for debugging (do not train) /\n",
      "                        デバッグ用に学習データを画面表示する（学習は行わない）\n",
      "  --resolution RESOLUTION\n",
      "                        resolution in training ('size' or 'width,height') /\n",
      "                        学習時の画像解像度（'サイズ'指定、または'幅,高さ'指定）\n",
      "  --cache_latents       cache latents to main memory to reduce VRAM usage\n",
      "                        (augmentations must be disabled) /\n",
      "                        VRAM削減のためにlatentをメインメモリにcacheする（augmentationは使用不可）\n",
      "  --vae_batch_size VAE_BATCH_SIZE\n",
      "                        batch size for caching latents / latentのcache時のバッチサイズ\n",
      "  --cache_latents_to_disk\n",
      "                        cache latents to disk to reduce VRAM usage\n",
      "                        (augmentations must be disabled) /\n",
      "                        VRAM削減のためにlatentをディスクにcacheする（augmentationは使用不可）\n",
      "  --enable_bucket       enable buckets for multi aspect ratio training /\n",
      "                        複数解像度学習のためのbucketを有効にする\n",
      "  --min_bucket_reso MIN_BUCKET_RESO\n",
      "                        minimum resolution for buckets / bucketの最小解像度\n",
      "  --max_bucket_reso MAX_BUCKET_RESO\n",
      "                        maximum resolution for buckets / bucketの最大解像度\n",
      "  --bucket_reso_steps BUCKET_RESO_STEPS\n",
      "                        steps of resolution for buckets, divisible by 8 is\n",
      "                        recommended / bucketの解像度の単位、8で割り切れる値を推奨します\n",
      "  --bucket_no_upscale   make bucket for each image without upscaling /\n",
      "                        画像を拡大せずbucketを作成します\n",
      "  --token_warmup_min TOKEN_WARMUP_MIN\n",
      "                        start learning at N tags (token means comma separated\n",
      "                        strinfloatgs) / タグ数をN個から増やしながら学習する\n",
      "  --token_warmup_step TOKEN_WARMUP_STEP\n",
      "                        tag length reaches maximum on N steps (or\n",
      "                        N*max_train_steps if N<1) / N（N<1ならN*max_train_steps）ス\n",
      "                        テップでタグ長が最大になる。デフォルトは0（最初から最大）\n",
      "  --dataset_class DATASET_CLASS\n",
      "                        dataset class for arbitrary dataset\n",
      "                        (package.module.Class) / 任意のデータセットを用いるときのクラス名\n",
      "                        (package.module.Class)\n",
      "  --caption_dropout_rate CAPTION_DROPOUT_RATE\n",
      "                        Rate out dropout caption(0.0~1.0) /\n",
      "                        captionをdropoutする割合\n",
      "  --caption_dropout_every_n_epochs CAPTION_DROPOUT_EVERY_N_EPOCHS\n",
      "                        Dropout all captions every N epochs /\n",
      "                        captionを指定エポックごとにdropoutする\n",
      "  --caption_tag_dropout_rate CAPTION_TAG_DROPOUT_RATE\n",
      "                        Rate out dropout comma separated tokens(0.0~1.0) /\n",
      "                        カンマ区切りのタグをdropoutする割合\n",
      "  --reg_data_dir REG_DATA_DIR\n",
      "                        directory for regularization images / 正則化画像データのディレクトリ\n",
      "  --in_json IN_JSON     json metadata for dataset / データセットのmetadataのjsonファイル\n",
      "  --dataset_repeats DATASET_REPEATS\n",
      "                        repeat dataset when training with captions /\n",
      "                        キャプションでの学習時にデータセットを繰り返す回数\n",
      "  --output_dir OUTPUT_DIR\n",
      "                        directory to output trained model / 学習後のモデル出力先ディレクトリ\n",
      "  --output_name OUTPUT_NAME\n",
      "                        base name of trained model file / 学習後のモデルの拡張子を除くファイル名\n",
      "  --huggingface_repo_id HUGGINGFACE_REPO_ID\n",
      "                        huggingface repo name to upload /\n",
      "                        huggingfaceにアップロードするリポジトリ名\n",
      "  --huggingface_repo_type HUGGINGFACE_REPO_TYPE\n",
      "                        huggingface repo type to upload /\n",
      "                        huggingfaceにアップロードするリポジトリの種類\n",
      "  --huggingface_path_in_repo HUGGINGFACE_PATH_IN_REPO\n",
      "                        huggingface model path to upload files /\n",
      "                        huggingfaceにアップロードするファイルのパス\n",
      "  --huggingface_token HUGGINGFACE_TOKEN\n",
      "                        huggingface token / huggingfaceのトークン\n",
      "  --huggingface_repo_visibility HUGGINGFACE_REPO_VISIBILITY\n",
      "                        huggingface repository visibility ('public' for\n",
      "                        public, 'private' or None for private) / huggingfaceにア\n",
      "                        ップロードするリポジトリの公開設定（'public'で公開、'private'またはNoneで非公開）\n",
      "  --save_state_to_huggingface\n",
      "                        save state to huggingface / huggingfaceにstateを保存する\n",
      "  --resume_from_huggingface\n",
      "                        resume from huggingface (ex: --resume\n",
      "                        {repo_id}/{path_in_repo}:{revision}:{repo_type}) /\n",
      "                        huggingfaceから学習を再開する(例: --resume\n",
      "                        {repo_id}/{path_in_repo}:{revision}:{repo_type})\n",
      "  --async_upload        upload to huggingface asynchronously /\n",
      "                        huggingfaceに非同期でアップロードする\n",
      "  --save_precision {None,float,fp16,bf16}\n",
      "                        precision in saving / 保存時に精度を変更して保存する\n",
      "  --save_every_n_epochs SAVE_EVERY_N_EPOCHS\n",
      "                        save checkpoint every N epochs / 学習中のモデルを指定エポックごとに保存する\n",
      "  --save_every_n_steps SAVE_EVERY_N_STEPS\n",
      "                        save checkpoint every N steps / 学習中のモデルを指定ステップごとに保存する\n",
      "  --save_n_epoch_ratio SAVE_N_EPOCH_RATIO\n",
      "                        save checkpoint N epoch ratio (for example 5 means\n",
      "                        save at least 5 files total) /\n",
      "                        学習中のモデルを指定のエポック割合で保存する（たとえば5を指定すると最低5個のファイルが保存される）\n",
      "  --save_last_n_epochs SAVE_LAST_N_EPOCHS\n",
      "                        save last N checkpoints when saving every N epochs\n",
      "                        (remove older checkpoints) /\n",
      "                        指定エポックごとにモデルを保存するとき最大Nエポック保存する（古いチェックポイントは削除する）\n",
      "  --save_last_n_epochs_state SAVE_LAST_N_EPOCHS_STATE\n",
      "                        save last N checkpoints of state (overrides the value\n",
      "                        of --save_last_n_epochs)/\n",
      "                        最大Nエポックstateを保存する（--save_last_n_epochsの指定を上書きする）\n",
      "  --save_last_n_steps SAVE_LAST_N_STEPS\n",
      "                        save checkpoints until N steps elapsed (remove older\n",
      "                        checkpoints if N steps elapsed) / 指定ステップごとにモデルを保存するとき、\n",
      "                        このステップ数経過するまで保存する（このステップ数経過したら削除する）\n",
      "  --save_last_n_steps_state SAVE_LAST_N_STEPS_STATE\n",
      "                        save states until N steps elapsed (remove older states\n",
      "                        if N steps elapsed, overrides --save_last_n_steps) / 指\n",
      "                        定ステップごとにstateを保存するとき、このステップ数経過するまで保存する（このステップ数経過したら削除す\n",
      "                        る。--save_last_n_stepsを上書きする）\n",
      "  --save_state          save training state additionally (including optimizer\n",
      "                        states etc.) / optimizerなど学習状態も含めたstateを追加で保存する\n",
      "  --resume RESUME       saved state to resume training / 学習再開するモデルのstate\n",
      "  --train_batch_size TRAIN_BATCH_SIZE\n",
      "                        batch size for training / 学習時のバッチサイズ\n",
      "  --max_token_length {None,150,225}\n",
      "                        max token length of text encoder (default for 75, 150\n",
      "                        or 225) / text encoderのトークンの最大長（未指定で75、150または225が指定可）\n",
      "  --mem_eff_attn        use memory efficient attention for CrossAttention /\n",
      "                        CrossAttentionに省メモリ版attentionを使う\n",
      "  --xformers            use xformers for CrossAttention /\n",
      "                        CrossAttentionにxformersを使う\n",
      "  --sdpa                use sdpa for CrossAttention (requires PyTorch 2.0) /\n",
      "                        CrossAttentionにsdpaを使う（PyTorch 2.0が必要）\n",
      "  --vae VAE             path to checkpoint of vae to replace /\n",
      "                        VAEを入れ替える場合、VAEのcheckpointファイルまたはディレクトリ\n",
      "  --max_train_steps MAX_TRAIN_STEPS\n",
      "                        training steps / 学習ステップ数\n",
      "  --max_train_epochs MAX_TRAIN_EPOCHS\n",
      "                        training epochs (overrides max_train_steps) /\n",
      "                        学習エポック数（max_train_stepsを上書きします）\n",
      "  --max_data_loader_n_workers MAX_DATA_LOADER_N_WORKERS\n",
      "                        max num workers for DataLoader (lower is less main RAM\n",
      "                        usage, faster epoch start and slower data loading) / D\n",
      "                        ataLoaderの最大プロセス数（小さい値ではメインメモリの使用量が減りエポック間の待ち時間が減りますが、\n",
      "                        データ読み込みは遅くなります）\n",
      "  --persistent_data_loader_workers\n",
      "                        persistent DataLoader workers (useful for reduce time\n",
      "                        gap between epoch, but may use more memory) /\n",
      "                        DataLoader のワーカーを持続させる\n",
      "                        (エポック間の時間差を少なくするのに有効だが、より多くのメモリを消費する可能性がある)\n",
      "  --seed SEED           random seed for training / 学習時の乱数のseed\n",
      "  --gradient_checkpointing\n",
      "                        enable gradient checkpointing / grandient\n",
      "                        checkpointingを有効にする\n",
      "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
      "                        Number of updates steps to accumulate before\n",
      "                        performing a backward/update pass /\n",
      "                        学習時に逆伝播をする前に勾配を合計するステップ数\n",
      "  --mixed_precision {no,fp16,bf16}\n",
      "                        use mixed precision / 混合精度を使う場合、その精度\n",
      "  --full_fp16           fp16 training including gradients / 勾配も含めてfp16で学習する\n",
      "  --full_bf16           bf16 training including gradients / 勾配も含めてbf16で学習する\n",
      "  --clip_skip CLIP_SKIP\n",
      "                        use output of nth layer from back of text encoder\n",
      "                        (n>=1) / text encoderの後ろからn番目の層の出力を用いる（nは1以上）\n",
      "  --logging_dir LOGGING_DIR\n",
      "                        enable logging and output TensorBoard log to this\n",
      "                        directory / ログ出力を有効にしてこのディレクトリにTensorBoard用のログを出力する\n",
      "  --log_with {tensorboard,wandb,all}\n",
      "                        what logging tool(s) to use (if 'all', TensorBoard and\n",
      "                        WandB are both used) / ログ出力に使用するツール\n",
      "                        (allを指定するとTensorBoardとWandBの両方が使用される)\n",
      "  --log_prefix LOG_PREFIX\n",
      "                        add prefix for each log directory /\n",
      "                        ログディレクトリ名の先頭に追加する文字列\n",
      "  --log_tracker_name LOG_TRACKER_NAME\n",
      "                        name of tracker to use for logging, default is script-\n",
      "                        specific default name /\n",
      "                        ログ出力に使用するtrackerの名前、省略時はスクリプトごとのデフォルト名\n",
      "  --log_tracker_config LOG_TRACKER_CONFIG\n",
      "                        path to tracker config file to use for logging /\n",
      "                        ログ出力に使用するtrackerの設定ファイルのパス\n",
      "  --wandb_api_key WANDB_API_KEY\n",
      "                        specify WandB API key to log in before starting\n",
      "                        training (optional). / WandB\n",
      "                        APIキーを指定して学習開始前にログインする（オプション）\n",
      "  --noise_offset NOISE_OFFSET\n",
      "                        enable noise offset with this value (if enabled,\n",
      "                        around 0.1 is recommended) / Noise\n",
      "                        offsetを有効にしてこの値を設定する（有効にする場合は0.1程度を推奨）\n",
      "  --multires_noise_iterations MULTIRES_NOISE_ITERATIONS\n",
      "                        enable multires noise with this number of iterations\n",
      "                        (if enabled, around 6-10 is recommended) / Multires\n",
      "                        noiseを有効にしてこのイテレーション数を設定する（有効にする場合は6-10程度を推奨）\n",
      "  --multires_noise_discount MULTIRES_NOISE_DISCOUNT\n",
      "                        set discount value for multires noise (has no effect\n",
      "                        without --multires_noise_iterations) / Multires noiseの\n",
      "                        discount値を設定する（--multires_noise_iterations指定時のみ有効）\n",
      "  --adaptive_noise_scale ADAPTIVE_NOISE_SCALE\n",
      "                        add `latent mean absolute value * this value` to\n",
      "                        noise_offset (disabled if None, default) /\n",
      "                        latentの平均値の絶対値 *\n",
      "                        この値をnoise_offsetに加算する（Noneの場合は無効、デフォルト）\n",
      "  --zero_terminal_snr   fix noise scheduler betas to enforce zero terminal SNR\n",
      "                        / noise schedulerのbetasを修正して、zero terminal SNRを強制する\n",
      "  --min_timestep MIN_TIMESTEP\n",
      "                        set minimum time step for U-Net training (0~999,\n",
      "                        default is 0) / U-Net学習時のtime\n",
      "                        stepの最小値を設定する（0~999で指定、省略時はデフォルト値(0)）\n",
      "  --max_timestep MAX_TIMESTEP\n",
      "                        set maximum time step for U-Net training (1~1000,\n",
      "                        default is 1000) / U-Net学習時のtime\n",
      "                        stepの最大値を設定する（1~1000で指定、省略時はデフォルト値(1000)）\n",
      "  --lowram              enable low RAM optimization. e.g. load models to VRAM\n",
      "                        instead of RAM (for machines which have bigger VRAM\n",
      "                        than RAM such as Colab and Kaggle) / メインメモリが少ない環境向け最適化\n",
      "                        を有効にする。たとえばVRAMにモデルを読み込むなど（ColabやKaggleなどRAMに比べてVRAMが多\n",
      "                        い環境向け）\n",
      "  --sample_every_n_steps SAMPLE_EVERY_N_STEPS\n",
      "                        generate sample images every N steps /\n",
      "                        学習中のモデルで指定ステップごとにサンプル出力する\n",
      "  --sample_every_n_epochs SAMPLE_EVERY_N_EPOCHS\n",
      "                        generate sample images every N epochs (overwrites\n",
      "                        n_steps) / 学習中のモデルで指定エポックごとにサンプル出力する（ステップ数指定を上書きします）\n",
      "  --sample_prompts SAMPLE_PROMPTS\n",
      "                        file for prompts to generate sample images /\n",
      "                        学習中モデルのサンプル出力用プロンプトのファイル\n",
      "  --sample_sampler {ddim,pndm,lms,euler,euler_a,heun,dpm_2,dpm_2_a,dpmsolver,dpmsolver++,dpmsingle,k_lms,k_euler,k_euler_a,k_dpm_2,k_dpm_2_a}\n",
      "                        sampler (scheduler) type for sample images /\n",
      "                        サンプル出力時のサンプラー（スケジューラ）の種類\n",
      "  --config_file CONFIG_FILE\n",
      "                        using .toml instead of args to pass hyperparameter /\n",
      "                        ハイパーパラメータを引数ではなく.tomlファイルで渡す\n",
      "  --output_config       output command line args to given .toml file /\n",
      "                        引数を.tomlファイルに出力する\n",
      "  --prior_loss_weight PRIOR_LOSS_WEIGHT\n",
      "                        loss weight for regularization images / 正則化画像のlossの重み\n",
      "  --optimizer_type OPTIMIZER_TYPE\n",
      "                        Optimizer to use / オプティマイザの種類: AdamW (default),\n",
      "                        AdamW8bit, PagedAdamW8bit, Lion8bit, PagedLion8bit,\n",
      "                        Lion, SGDNesterov, SGDNesterov8bit,\n",
      "                        DAdaptation(DAdaptAdamPreprint), DAdaptAdaGrad,\n",
      "                        DAdaptAdam, DAdaptAdan, DAdaptAdanIP, DAdaptLion,\n",
      "                        DAdaptSGD, AdaFactor\n",
      "  --use_8bit_adam       use 8bit AdamW optimizer (requires bitsandbytes) /\n",
      "                        8bit Adamオプティマイザを使う（bitsandbytesのインストールが必要）\n",
      "  --use_lion_optimizer  use Lion optimizer (requires lion-pytorch) /\n",
      "                        Lionオプティマイザを使う（ lion-pytorch のインストールが必要）\n",
      "  --learning_rate LEARNING_RATE\n",
      "                        learning rate / 学習率\n",
      "  --max_grad_norm MAX_GRAD_NORM\n",
      "                        Max gradient norm, 0 for no clipping /\n",
      "                        勾配正規化の最大norm、0でclippingを行わない\n",
      "  --optimizer_args [OPTIMIZER_ARGS ...]\n",
      "                        additional arguments for optimizer (like\n",
      "                        \"weight_decay=0.01 betas=0.9,0.999 ...\") /\n",
      "                        オプティマイザの追加引数（例： \"weight_decay=0.01 betas=0.9,0.999\n",
      "                        ...\"）\n",
      "  --lr_scheduler_type LR_SCHEDULER_TYPE\n",
      "                        custom scheduler module / 使用するスケジューラ\n",
      "  --lr_scheduler_args [LR_SCHEDULER_ARGS ...]\n",
      "                        additional arguments for scheduler (like \"T_max=100\")\n",
      "                        / スケジューラの追加引数（例： \"T_max100\"）\n",
      "  --lr_scheduler LR_SCHEDULER\n",
      "                        scheduler to use for learning rate / 学習率のスケジューラ:\n",
      "                        linear, cosine, cosine_with_restarts, polynomial,\n",
      "                        constant (default), constant_with_warmup, adafactor\n",
      "  --lr_warmup_steps LR_WARMUP_STEPS\n",
      "                        Number of steps for the warmup in the lr scheduler\n",
      "                        (default is 0) / 学習率のスケジューラをウォームアップするステップ数（デフォルト0）\n",
      "  --lr_scheduler_num_cycles LR_SCHEDULER_NUM_CYCLES\n",
      "                        Number of restarts for cosine scheduler with restarts\n",
      "                        / cosine with restartsスケジューラでのリスタート回数\n",
      "  --lr_scheduler_power LR_SCHEDULER_POWER\n",
      "                        Polynomial power for polynomial scheduler /\n",
      "                        polynomialスケジューラでのpolynomial power\n",
      "  --dataset_config DATASET_CONFIG\n",
      "                        config file for detail settings / 詳細な設定用の設定ファイル\n",
      "  --min_snr_gamma MIN_SNR_GAMMA\n",
      "                        gamma for reducing the weight of high loss timesteps.\n",
      "                        Lower numbers have stronger effect. 5 is recommended\n",
      "                        by paper. / 低いタイムステップでの高いlossに対して重みを減らすためのgamma値、低いほど効\n",
      "                        果が強く、論文では5が推奨\n",
      "  --scale_v_pred_loss_like_noise_pred\n",
      "                        scale v-prediction loss like noise prediction loss /\n",
      "                        v-prediction lossをnoise prediction lossと同じようにスケーリングする\n",
      "  --weighted_captions   Enable weighted captions in the standard style\n",
      "                        (token:1.3). No commas inside parens, or\n",
      "                        shuffle/dropout may break the decoder. / 「[token]」、「(t\n",
      "                        oken)」「(token:1.3)」のような重み付きキャプションを有効にする。カンマを括弧内に入れるとシャ\n",
      "                        ッフルやdropoutで重みづけがおかしくなるので注意\n",
      "  --no_metadata         do not save metadata in output model /\n",
      "                        メタデータを出力先モデルに保存しない\n",
      "  --save_model_as {None,ckpt,pt,safetensors}\n",
      "                        format to save the model (default is .safetensors) /\n",
      "                        モデル保存時の形式（デフォルトはsafetensors）\n",
      "  --unet_lr UNET_LR     learning rate for U-Net / U-Netの学習率\n",
      "  --text_encoder_lr TEXT_ENCODER_LR\n",
      "                        learning rate for Text Encoder / Text Encoderの学習率\n",
      "  --network_weights NETWORK_WEIGHTS\n",
      "                        pretrained weights for network / 学習するネットワークの初期重み\n",
      "  --network_module NETWORK_MODULE\n",
      "                        network module to train / 学習対象のネットワークのモジュール\n",
      "  --network_dim NETWORK_DIM\n",
      "                        network dimensions (depends on each network) /\n",
      "                        モジュールの次元数（ネットワークにより定義は異なります）\n",
      "  --network_alpha NETWORK_ALPHA\n",
      "                        alpha for LoRA weight scaling, default 1 (same as\n",
      "                        network_dim for same behavior as old version) / LoRaの重\n",
      "                        み調整のalpha値、デフォルト1（旧バージョンと同じ動作をするにはnetwork_dimと同じ値を指定）\n",
      "  --network_dropout NETWORK_DROPOUT\n",
      "                        Drops neurons out of training every step (0 or None is\n",
      "                        default behavior (no dropout), 1 would drop all\n",
      "                        neurons) / 訓練時に毎ステップでニューロンをdropする（0またはNoneはdropoutなし、1\n",
      "                        は全ニューロンをdropout）\n",
      "  --network_args [NETWORK_ARGS ...]\n",
      "                        additional argmuments for network (key=value) /\n",
      "                        ネットワークへの追加の引数\n",
      "  --network_train_unet_only\n",
      "                        only training U-Net part / U-Net関連部分のみ学習する\n",
      "  --network_train_text_encoder_only\n",
      "                        only training Text Encoder part / Text\n",
      "                        Encoder関連部分のみ学習する\n",
      "  --training_comment TRAINING_COMMENT\n",
      "                        arbitrary comment string stored in metadata /\n",
      "                        メタデータに記録する任意のコメント文字列\n",
      "  --dim_from_weights    automatically determine dim (rank) from\n",
      "                        network_weights / dim\n",
      "                        (rank)をnetwork_weightsで指定した重みから自動で決定する\n",
      "  --scale_weight_norms SCALE_WEIGHT_NORMS\n",
      "                        Scale the weight of each key pair to help prevent\n",
      "                        overtraing via exploding gradients. (1 is a good\n",
      "                        starting point) / 重みの値をスケーリングして勾配爆発を防ぐ（1が初期値としては適当）\n",
      "  --base_weights [BASE_WEIGHTS ...]\n",
      "                        network weights to merge into the model before\n",
      "                        training / 学習前にあらかじめモデルにマージするnetworkの重みファイル\n",
      "  --base_weights_multiplier [BASE_WEIGHTS_MULTIPLIER ...]\n",
      "                        multiplier for network weights to merge into the model\n",
      "                        before training / 学習前にあらかじめモデルにマージするnetworkの重みの倍率\n",
      "  --no_half_vae         do not use fp16/bf16 VAE in mixed precision (use float\n",
      "                        VAE) / mixed precisionでも fp16/bf16 VAEを使わずfloat VAEを使う\n",
      "  --cache_text_encoder_outputs\n",
      "                        cache text encoder outputs / text encoderの出力をキャッシュする\n",
      "  --cache_text_encoder_outputs_to_disk\n",
      "                        cache text encoder outputs to disk / text\n",
      "                        encoderの出力をディスクにキャッシュする\n"
     ]
    }
   ],
   "source": [
    "!python sd-scripts/sdxl_train_network.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa33f0f3-4495-42e3-a826-0d73d6807082",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "Loading settings from kohya_configs/config.toml...\n",
      "kohya_configs/config\n",
      "prepare tokenizers\n",
      "Loading dataset config from kohya_configs/config_phil.toml\n",
      "prepare images.\n",
      "found directory /workspace/sd-xl-experiments/data/phil_1024 contains 11 image files\n",
      "No caption file found for 11 images. Training will continue without captions for these images. If class token exists, it will be used. / 11枚の画像にキャプションファイルが見つかりませんでした。これらの画像についてはキャプションなしで学習を続行します。class tokenが存在する場合はそれを使います。\n",
      "/workspace/sd-xl-experiments/data/phil_1024/shs (1).jpg\n",
      "/workspace/sd-xl-experiments/data/phil_1024/shs (10).jpg\n",
      "/workspace/sd-xl-experiments/data/phil_1024/shs (2).jpg\n",
      "/workspace/sd-xl-experiments/data/phil_1024/shs (3).jpg\n",
      "/workspace/sd-xl-experiments/data/phil_1024/shs (4).jpg\n",
      "/workspace/sd-xl-experiments/data/phil_1024/shs (5).jpg... and 6 more\n",
      "110 train images with repeating.\n",
      "0 reg images.\n",
      "no regularization images / 正則化画像が見つかりませんでした\n",
      "[Dataset 0]\n",
      "  batch_size: 1\n",
      "  resolution: (1024, 1024)\n",
      "  enable_bucket: False\n",
      "\n",
      "  [Subset 0 of Dataset 0]\n",
      "    image_dir: \"/workspace/sd-xl-experiments/data/phil_1024\"\n",
      "    image_count: 11\n",
      "    num_repeats: 10\n",
      "    shuffle_caption: False\n",
      "    keep_tokens: 0\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    is_reg: False\n",
      "    class_tokens: shs man\n",
      "    caption_extension: .caption\n",
      "\n",
      "\n",
      "[Dataset 0]\n",
      "loading image sizes.\n",
      "100%|█████████████████████████████████████████| 11/11 [00:00<00:00, 1475.45it/s]\n",
      "prepare dataset\n",
      "noise_offset is set to 0.0357 / noise_offsetが0.0357に設定されました\n",
      "preparing accelerator\n",
      "loading model for process 0/1\n",
      "load Diffusers pretrained models: stabilityai/stable-diffusion-xl-base-1.0, variant=None\n",
      "The config attributes {'force_upcast': True} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "U-Net converted to original U-Net\n",
      "Enable xformers for U-Net\n",
      "import network module: networks.lora\n",
      "[Dataset 0]\n",
      "caching latents.\n",
      "checking cache validity...\n",
      "100%|███████████████████████████████████████| 11/11 [00:00<00:00, 216607.25it/s]\n",
      "caching latents...\n",
      "100%|███████████████████████████████████████████| 11/11 [00:05<00:00,  1.90it/s]\n",
      "create LoRA network. base dim (rank): 128, alpha: 1\n",
      "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
      "create LoRA for Text Encoder 1:\n",
      "create LoRA for Text Encoder 2:\n",
      "create LoRA for Text Encoder: 264 modules.\n",
      "create LoRA for U-Net: 722 modules.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "prepare optimizer, data loader etc.\n",
      "use Adafactor optimizer | {'scale_parameter': False, 'relative_step': False, 'warmup_init': False}\n",
      "because max_grad_norm is set, clip_grad_norm is enabled. consider set to 0 / max_grad_normが設定されているためclip_grad_normが有効になります。0に設定して無効にしたほうがいいかもしれません\n",
      "constant_with_warmup will be good / スケジューラはconstant_with_warmupが良いかもしれません\n",
      "running training / 学習開始\n",
      "  num train images * repeats / 学習画像の数×繰り返し回数: 110\n",
      "  num reg images / 正則化画像の数: 0\n",
      "  num batches per epoch / 1epochのバッチ数: 110\n",
      "  num epochs / epoch数: 10\n",
      "  batch size per device / バッチサイズ: 1\n",
      "  gradient accumulation steps / 勾配を合計するステップ数 = 1\n",
      "  total optimization steps / 学習ステップ数: 1000\n",
      "steps:   0%|                                           | 0/1000 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphilippe-henri-saade\u001b[0m (\u001b[33mkollai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1mmodels/phil_1000_lr4e-5_cosine_restarts_10_dim_12820230729102922/wandb/run-20230729_103023-eao49m61\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msmart-sunset-10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kollai/network_train\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kollai/network_train/runs/eao49m61\u001b[0m\n",
      "\n",
      "epoch 1/10\n",
      "/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "steps:  11%|██▍                   | 110/1000 [03:28<28:09,  1.90s/it, loss=0.11]\n",
      "epoch 2/10\n",
      "steps:  16%|███▍                 | 162/1000 [05:04<26:12,  1.88s/it, loss=0.119]"
     ]
    }
   ],
   "source": [
    "! accelerate launch sd-scripts/sdxl_train_network.py --config=kohya_configs/config.toml --dataset_config=kohya_configs/config_phil.toml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
